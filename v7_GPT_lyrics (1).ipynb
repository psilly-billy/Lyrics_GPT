{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNkk8hG2FnV0"
   },
   "source": [
    "# Building a lyrics GPT \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSmV-VaRpT3h",
    "outputId": "bb82e04e-6466-4680-c7f1-05bf36433c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Quadro M4000 (UUID: GPU-a6fd9309-8a85-fb51-3725-dfa45cb9c348)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d8vFRi8K9tcw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em4PbpchF16a"
   },
   "source": [
    "## Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY7EolzQ9kyw",
    "outputId": "b78ec8b7-a48a-4296-a319-4b078da6671a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-27 11:48:55--  https://raw.githubusercontent.com/psilly-billy/Lyrics_GPT/main/lyrics_dataset.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 915130 (894K) [text/plain]\n",
      "Saving to: ‘lyrics_dataset.txt’\n",
      "\n",
      "lyrics_dataset.txt  100%[===================>] 893.68K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2023-03-27 11:48:55 (25.3 MB/s) - ‘lyrics_dataset.txt’ saved [915130/915130]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we will use all lyrics from Romanian hip-hop band B.U.G. Mafia, Poems by Eminescu and Bacovia\n",
    "!wget https://raw.githubusercontent.com/psilly-billy/Lyrics_GPT/main/lyrics_dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4EBJldzGukn"
   },
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-hX3dsnm9kv-"
   },
   "outputs": [],
   "source": [
    "with open ('lyrics_dataset.txt', 'r', encoding = 'utf-8') as f:\n",
    "  text = f.read()\n",
    "\n",
    "  #read data and inspect, 'r' - for reading, create a dataset 'text' from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY1M6kAx9ks_",
    "outputId": "9721ad23-a36e-4db5-fb3e-8c60decf75e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length of dataset:  878926\n"
     ]
    }
   ],
   "source": [
    "print (\"Character Length of dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FySrVxC09kqS",
    "outputId": "fda0c6e6-ea4d-450d-c9d1-b4d808505e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " B. U. G. Mafia - Şi cui ii pasă\n",
      "\n",
      "Caddillac:\n",
      "Ai un Mercedes de moare lumea, dar n-ai clasă ca şi el,\n",
      "Dacă te-ar vedea patronii, ar renega acest model..\n",
      "Vorbeşti la Vertu, cu vreo somitate, la pertú,\n",
      "Tare, să vadă lumea cine eşti tu..\n",
      "Dar cine eşti tu? N-ai nimic de arătat,\n",
      "Doar ambalaj strălucitor s-ascundă mult căcat..\n",
      "Adevărat, e tare p***a ta,\n",
      "Vreo trei ar da la ea, fiindcă restu' au dat deja..\n",
      "Şi, deşi n-are nimic în cap, se vrea a fi vedetă,\n",
      "Tu bagi banii ca-n depozit că vrea etichetă,\n",
      "E plină de silicon, din subsol până-n balcon,\n",
      "Ai grijă, nu umbla cu acu' pe lângă balon..\n",
      "Şi nu uita să cotizezi la băieţi, în cluburi,\n",
      "Că s-ar putea ca alţi băieţi să te scoată-n şuturi..\n",
      "Acum, n-o lua personal, am dat doar un exemplu,\n",
      "Oricum, sunt destui lingăi care să-ţi facă templu, nu?.\n",
      "\n",
      "Tataee:\n",
      "Eşti o prinţesă.. Şi cui îi pasă?\n",
      "Te dai în presă.. Şi cui îi pasă?\n",
      "Că eşti la modă.. Şi cui îi pasă?\n",
      "Hai, dă-te dracu', că nu ne pasă..\n",
      "Băiatu' tatii.. Şi cui îi pasă?\n",
      "Arunci cu banii.. Şi cui îi pasă?\n"
     ]
    }
   ],
   "source": [
    "# look at first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8RlWWkG9knT",
    "outputId": "0907018e-2e72-4bd2-ceb0-04536813a813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '¬', '³', 'Â', 'Î', 'Ñ', 'à', 'â', 'í', 'î', 'ï', 'ö', 'ú', 'Ă', 'ă', 'Ŕ', 'Ş', 'ş', 'Ţ', 'ţ', 'Ș', 'ș', 'ț', '—', '’', '“', '”', '„', '‟', '…']\n",
      "\t\n",
      " !\"%&'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz~¬³ÂÎÑàâíîïöúĂăŔŞşŢţȘșț—’“”„‟…\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "# How many unique characters we have in this dataset?\n",
    "chars = sorted(list(set(text))) # call a 'set' of all the characters that are in this dataset, make a 'list' out of it and after sort that\n",
    "\n",
    "vocab_char = len(chars) #how many unique characters the model can see and use\n",
    "\n",
    "print(chars)\n",
    "print(\"\".join(chars))\n",
    "print (vocab_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa25J4MPMwJO"
   },
   "source": [
    "### Removing some characters and replacing some words \n",
    "The data has some words that are censored and some other characters thata we want to take out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p_U-2Qe69kkm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the words you want to replace\n",
    "words_to_replace = {\"Ñ\":\"N\",\"í\":\"i\",\"ï\":\"i\",\"ú\":\"u\",\"p***a\":\"pizda\",\n",
    "                    \"p**a\":\"pula\", \"p*z*a\":\"pizda\",\n",
    "                    \"ga*aza\":\"gaoaza\", \"m**e\":\"muie\",\n",
    "                    \"c***t\":\"cacat\", \"p***\":\"pula\", \"f*ă\":\"futa\",\n",
    "                    \"sl***z\":\"sloboz\", \"f*teti\":\"futeti\", \"c***t\":\"cacat\", \"cuprins\":\" \",\n",
    "                    \"George\":\" \", \"Bacovia\":\" \", \"Plumb\":\" \"\n",
    "                    }\n",
    "\n",
    "for old_word,new_word in words_to_replace.items():\n",
    "    text= text.replace(old_word,new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wbH1hPsU9kht"
   },
   "outputs": [],
   "source": [
    "# Let's clean aout dataset of unwanted characters\n",
    "import re\n",
    "chars_to_remove = \"[%&=³‟*\\\\^|~£§©¬—■_@+/$`()“”„;...,?‘’…]\"  # Specify the characters we want to remove\n",
    "\n",
    "\n",
    "# Create a translation table\n",
    "trans_table = text.maketrans(\"\", \"\", chars_to_remove)\n",
    "\n",
    "\n",
    "# Use the translate method to remove the characters\n",
    "cleaned_text = text.translate(trans_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbvOU149_OaH"
   },
   "source": [
    "This code uses the `string.maketrans()` method to create a translation table that maps each character in the characters_to_remove string to None.\n",
    "Then, it uses the `str.translate()` method to remove the unwanted characters from the original string by applying the translation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcWJw3qJ9ket",
    "outputId": "163be2b4-20b4-4cd6-fa9d-71bdf43fbea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Â', 'Î', 'à', 'â', 'î', 'ö', 'Ă', 'ă', 'Ŕ', 'Ş', 'ş', 'Ţ', 'ţ', 'Ș', 'ș', 'ț']\n",
      "\t\n",
      " !\"'-0123456789:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÂÎàâîöĂăŔŞşŢţȘșț\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(cleaned_text))) # call a 'set' of all the characters that are in this dataset, make a 'list' out of it and after sort that\n",
    "\n",
    "vocab_char = len(chars) #how many unique characters the model can see and use\n",
    "\n",
    "print(chars)\n",
    "print(\"\".join(chars))\n",
    "print (vocab_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-gl8O0jNpOg"
   },
   "source": [
    "## Creating a new file with cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WM6r5ipe9kbe"
   },
   "outputs": [],
   "source": [
    "file_name = \"cleaned_text.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_name, \"w\") as f:\n",
    "    # Write the string to the file\n",
    "    f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYpV62g_9kY1",
    "outputId": "237e269b-26d0-4505-9da1-ccf0b0e999dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " B U G Mafia - Şi cui ii pasă\n",
      "\n",
      "Caddillac:\n",
      "Ai un Mercedes de moare lumea dar n-ai clasă ca şi el\n",
      "Dacă te-ar vedea patronii ar renega acest model\n",
      "Vorbeşti la Vertu cu vreo somitate la pertu\n",
      "Tare să vadă lumea cine eşti tu\n",
      "Dar cine eşti tu N-ai nimic de arătat\n",
      "Doar ambalaj strălucitor s-ascundă mult căcat\n",
      "Adevărat e tare pizda ta\n",
      "Vreo trei ar da la ea fiindcă restu' au dat deja\n",
      "Şi deşi n-are nimic în cap se vrea a fi vedetă\n",
      "Tu bagi banii ca-n depozit că vrea etichetă\n",
      "E plină de silicon din subsol până-n balcon\n",
      "Ai grijă nu umbla cu acu' pe lângă balon\n",
      "Şi nu uita să cotizezi la băieţi în cluburi\n",
      "Că s-ar putea ca alţi băieţi să te scoată-n şuturi\n",
      "Acum n-o lua personal am dat doar un exemplu\n",
      "Oricum sunt destui lingăi care să-ţi facă templu nu\n",
      "\n",
      "Tataee:\n",
      "Eşti o prinţesă Şi cui îi pasă\n",
      "Te dai în presă Şi cui îi pasă\n",
      "Că eşti la modă Şi cui îi pasă\n",
      "Hai dă-te dracu' că nu ne pasă\n",
      "Băiatu' tatii Şi cui îi pasă\n",
      "Arunci cu banii Şi cui îi pasă\n",
      "Eşti un spectacol Şi cui îi pasă\n",
      "Hai dă-te dracu' că nu ne pa\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajdljNrlL1UM",
    "outputId": "7d156e77-2341-4e86-8555-05e22b18ae7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, 128)]       0         \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, None, 128)        198272    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,272\n",
      "Trainable params: 198,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# Usage example\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "rate = 0.1\n",
    "\n",
    "sample_transformer_block = TransformerBlock(d_model, num_heads, dff, rate)\n",
    "\n",
    "input_shape = (None, d_model)\n",
    "sample_input = tf.keras.Input(shape=input_shape)\n",
    "sample_output = sample_transformer_block(sample_input, training=False, mask=None)\n",
    "\n",
    "sample_transformer_model = tf.keras.Model(inputs=sample_input, outputs=sample_output)\n",
    "print(sample_transformer_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNLcLx_-L1Pj",
    "outputId": "42db493b-4c7c-4346-de1e-5145674317a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 22)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 22, 512)           10276352  \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 22, 512)          1577984   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 22, 512)           0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 22, 20071)         10296423  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,150,759\n",
      "Trainable params: 22,150,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 681s 629ms/step - loss: 0.6750 - accuracy: 0.8894 - val_loss: 0.2539 - val_accuracy: 0.9382\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 681s 631ms/step - loss: 0.1760 - accuracy: 0.9502 - val_loss: 0.2030 - val_accuracy: 0.9514\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 680s 630ms/step - loss: 0.1251 - accuracy: 0.9624 - val_loss: 0.1880 - val_accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 694s 643ms/step - loss: 0.1014 - accuracy: 0.9695 - val_loss: 0.1803 - val_accuracy: 0.9604\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 693s 642ms/step - loss: 0.0861 - accuracy: 0.9744 - val_loss: 0.1698 - val_accuracy: 0.9641\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().splitlines()\n",
    "    return text\n",
    "\n",
    "file_path = \"cleaned_text.txt\"\n",
    "text_data = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Tokenize your dataset\n",
    "vocab_size = 20077\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences and labels\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='post')\n",
    "\n",
    "input_data = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, 1:]\n",
    "#labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# Build the model using the custom TransformerBlock\n",
    "d_model = 512\n",
    "num_heads = 16\n",
    "dff = 512\n",
    "rate = 0.1\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(max_sequence_len - 1,))\n",
    "embedding = tf.keras.layers.Embedding(total_words, d_model)(inputs)\n",
    "transformer_block = TransformerBlock(d_model, num_heads, dff, rate)(embedding)\n",
    "dropout = tf.keras.layers.Dropout(rate)(transformer_block)\n",
    "outputs = tf.keras.layers.Dense(total_words, activation='softmax')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "# Custom loss function\n",
    "def masked_categorical_crossentropy(y_true, y_pred):\n",
    "    mask = tf.cast(y_true[:, :, -1], tf.bool)\n",
    "    y_true = y_true[:, :, :-1]\n",
    "\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss=masked_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(input_data, labels, batch_size, total_words):\n",
    "    data_len = len(input_data)\n",
    "    num_batches = data_len // batch_size\n",
    "\n",
    "    while True:\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            x_batch = input_data[start:end]\n",
    "            y_batch = labels[start:end]\n",
    "            y_batch = tf.keras.utils.to_categorical(y_batch, num_classes=total_words)\n",
    "\n",
    "            # Create a mask for padding\n",
    "            mask = (y_batch != 0).any(axis=-1).astype(float)\n",
    "\n",
    "            # Append the mask to y_batch as the last element\n",
    "            y_batch = np.concatenate((y_batch, np.expand_dims(mask, -1)), axis=-1)\n",
    "\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_data_train, input_data_val, labels_train, labels_val = train_test_split(\n",
    "    input_data, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Create a generator with a batch size\n",
    "batch_size = 128\n",
    "train_generator = data_generator(input_data_train, labels_train, batch_size, total_words)\n",
    "val_generator = data_generator(input_data_val, labels_val, batch_size, total_words)\n",
    "\n",
    "# Compute the number of steps per epoch\n",
    "steps_per_epoch = len(input_data) // batch_size\n",
    "val_steps_per_epoch = len(input_data_val) // batch_size\n",
    "\n",
    "# Train the model using the generator\n",
    "#history = model.fit(train_generator, epochs=2, steps_per_epoch=steps_per_epoch, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WFUWS2Y7zXx5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_2_layer_call_fn, multi_head_attention_2_layer_call_and_return_conditional_losses, feed_forward_network_2_layer_call_fn, feed_forward_network_2_layer_call_and_return_conditional_losses, layer_normalization_4_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_5/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "tf.saved_model.save(model, 'saved_model_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v5_model.ckpt-1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a checkpoint object and save the model\n",
    "checkpoint = tf.train.Checkpoint(my_model=model)\n",
    "checkpoint.save('v5_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mBFpq95jDt1W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature=0.1):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for i in range(next_words):\n",
    "        # Split the output text into words\n",
    "        words = output_text.split()\n",
    "        \n",
    "        # Use the last 5 words as the seed text\n",
    "        if len(words) >= 5:\n",
    "            seed_text = \" \".join(words[-5:])\n",
    "        else:\n",
    "            seed_text = \" \".join(words)\n",
    "        \n",
    "        # Tokenize and pad the input sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_probs = model.predict(token_list)[0][-1]\n",
    "\n",
    "        # Apply temperature to the predicted probabilities\n",
    "        exp_preds = np.exp(np.log(predicted_probs) / temperature)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        # Normalize the probabilities using softmax\n",
    "        predicted_probs = K.softmax(predicted_probs).numpy()\n",
    "\n",
    "        # Select a word index based on the probability distribution\n",
    "        #predicted = np.random.choice(range(total_words), p=predicted_probs)\n",
    "        predicted = np.argmax(predicted_probs)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Append the generated word to the output text\n",
    "        output_text += \" \" + output_word\n",
    "        \n",
    "        # Add a line break after every 6th word\n",
    "        if (i+1) % 6 == 0:\n",
    "            output_text += \"\\n\"\n",
    "\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 20077\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "loaded_model = tf.saved_model.load('saved_model_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input signature: serving_default\n",
      "Output signature: {'dense_12': TensorShape([None, 126, 19038])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = tf.saved_model.load('my_saved_model', tags=[tag_constants.SERVING])\n",
    "\n",
    "# Get the input and output signatures\n",
    "input_signature = list(loaded_model.signatures.keys())[0]\n",
    "output_signature = loaded_model.signatures[input_signature].output_shapes\n",
    "\n",
    "# Print the signatures\n",
    "print(f\"Input signature: {input_signature}\")\n",
    "print(f\"Output signature: {output_signature}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature=5):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for i in range(next_words):\n",
    "        # Split the output text into words\n",
    "        words = output_text.split()\n",
    "        \n",
    "        # Use the last 5 words as the seed text\n",
    "        if len(words) >= 5:\n",
    "            seed_text = \" \".join(words[-5:])\n",
    "        else:\n",
    "            seed_text = \" \".join(words)\n",
    "        \n",
    "        # Tokenize and pad the input sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='post')\n",
    "        token_list = tf.cast(token_list, dtype=tf.float32)\n",
    "        \n",
    "        #print(\"Token List:\",token_list)\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_probs = model(token_list)[0][-1]\n",
    "        \n",
    "        #print(\"First predictions:\", predicted_probs)\n",
    "    \n",
    "        # Apply temperature to the predicted probabilities\n",
    "        exp_preds = np.exp(np.log(predicted_probs) / temperature)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        #print(\"Probabilities with temperature applied:\", predicted_probs)\n",
    "        \n",
    "        # Normalize the probabilities using softmax\n",
    "        predicted_probs = K.softmax(predicted_probs).numpy()\n",
    "        \n",
    "        #print(\"Normalized Predictions:\", predicted_probs)\n",
    "        \n",
    "        # Select a word index based on the probability distribution\n",
    "        predicted = np.random.choice(range(total_words), p=predicted_probs)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #print(\"Final Prediction:\", predicted)\n",
    "        \n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Append the generated word to the output text\n",
    "        output_text += \" \" + output_word\n",
    "        \n",
    "        # Add a line break after every 6th word\n",
    "        if (i+1) % 6 == 0:\n",
    "            output_text += \"\\n\"\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08KMZViqEAEv",
    "outputId": "1765fa93-c9bb-4ef4-d487-46e6039c65f4"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m next_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     17\u001b[0m max_sequence_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m127\u001b[39m\n\u001b[0;32m---> 18\u001b[0m new_lyrics \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_lyrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_lyrics)\n",
      "Cell \u001b[0;32mIn [8], line 41\u001b[0m, in \u001b[0;36mgenerate_lyrics\u001b[0;34m(seed_text, next_words, model, max_sequence_len, temperature)\u001b[0m\n\u001b[1;32m     36\u001b[0m predicted_probs \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msoftmax(predicted_probs)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#print(\"Normalized Predictions:\", predicted_probs)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Select a word index based on the probability distribution\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#print(\"Final Prediction:\", predicted)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m output_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32mmtrand.pyx:932\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "file_path = \"cleaned_text.txt\"\n",
    "# Load and preprocess your dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().splitlines()\n",
    "    return text\n",
    "text_data = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Tokenize your dataset\n",
    "vocab_size = 19038\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "seed_text = \"de dimineata\"\n",
    "next_words = 5\n",
    "max_sequence_len = 127\n",
    "new_lyrics = generate_lyrics(seed_text, next_words, loaded_model, max_sequence_len)\n",
    "print(new_lyrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddaAsN-s1fAi",
    "outputId": "b709071f-76d8-470d-bc79-cd392b038ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "de dimineata as merge sa fa ceva mântuitor chinul tipatul stranse nespus botosani focurile sii olace singur pitic ciudatele argintul stupul roma\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature=0.5):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        # Tokenize and pad the input sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_probs = model.predict(token_list)[0][-1]\n",
    "        #print(predicted_probs.shape)\n",
    "        #print(predicted_probs)\n",
    "\n",
    "\n",
    "        # Apply temperature to the predicted probabilities\n",
    "        exp_preds = np.exp(np.log(predicted_probs) / temperature)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        #print(predicted_probs)\n",
    "\n",
    "        # Normalize the probabilities using softmax\n",
    "        predicted_probs = K.softmax(predicted_probs).numpy()\n",
    "\n",
    "        # Select a word index based on the probability distribution\n",
    "        predicted = np.random.choice(range(total_words), p=predicted_probs)\n",
    "       # print(range(vocab_size))\n",
    "        #print(predicted)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Append the generated word to the seed text\n",
    "        seed_text += \" \" + output_word\n",
    "        output_text += \" \" + output_word\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "seed_text = \"de dimineata as merge sa fa ceva\"\n",
    "next_words = 15\n",
    "#vocab_size = 19038\n",
    "temperature = 10\n",
    "new_lyrics = generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature)\n",
    "print(new_lyrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wr2JIRlsL1KZ",
    "outputId": "1ea2aa36-b9ea-4dab-9caf-541ec7d2a0b8"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21g32tCHL1Hw",
    "outputId": "616e37e9-abea-4690-9985-7e7322cda309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20071\n"
     ]
    }
   ],
   "source": [
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXhq0fWCL1FV"
   },
   "source": [
    "Continue training from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sDXvdvGFL03w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).save_counter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).my_model.optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).my_model.optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).my_model.optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).my_model.optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).my_model.optimizer.learning_rate\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 22)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 22, 256)           5138176   \n",
      "                                                                 \n",
      " transformer_block_8 (Transf  (None, 22, 256)          527104    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 22, 256)           0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 22, 20071)         5158247   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,823,527\n",
      "Trainable params: 10,823,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().splitlines()\n",
    "    return text\n",
    "\n",
    "file_path = \"cleaned_text.txt\"\n",
    "text_data = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Tokenize your dataset\n",
    "vocab_size = 20071\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences and labels\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='post')\n",
    "\n",
    "input_data = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, 1:]\n",
    "#labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "# Build the model using the custom TransformerBlock\n",
    "d_model = 256\n",
    "num_heads = 16\n",
    "dff = 512\n",
    "rate = 0.1\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(max_sequence_len - 1,))\n",
    "embedding = tf.keras.layers.Embedding(total_words, d_model)(inputs)\n",
    "transformer_block = TransformerBlock(d_model, num_heads, dff, rate)(embedding)\n",
    "dropout = tf.keras.layers.Dropout(rate)(transformer_block)\n",
    "outputs = tf.keras.layers.Dense(total_words, activation='softmax')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "# Custom loss function\n",
    "def masked_categorical_crossentropy(y_true, y_pred):\n",
    "    mask = tf.cast(y_true[:, :, -1], tf.bool)\n",
    "    y_true = y_true[:, :, :-1]\n",
    "\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss=masked_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(input_data, labels, batch_size, total_words):\n",
    "    data_len = len(input_data)\n",
    "    num_batches = data_len // batch_size\n",
    "\n",
    "    while True:\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            x_batch = input_data[start:end]\n",
    "            y_batch = labels[start:end]\n",
    "            y_batch = tf.keras.utils.to_categorical(y_batch, num_classes=total_words)\n",
    "\n",
    "            # Create a mask for padding\n",
    "            mask = (y_batch != 0).any(axis=-1).astype(float)\n",
    "\n",
    "            # Append the mask to y_batch as the last element\n",
    "            y_batch = np.concatenate((y_batch, np.expand_dims(mask, -1)), axis=-1)\n",
    "\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_data_train, input_data_val, labels_train, labels_val = train_test_split(\n",
    "    input_data, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Create a generator with a batch size\n",
    "batch_size = 128\n",
    "train_generator = data_generator(input_data_train, labels_train, batch_size, total_words)\n",
    "val_generator = data_generator(input_data_val, labels_val, batch_size, total_words)\n",
    "\n",
    "# Compute the number of steps per epoch\n",
    "steps_per_epoch = len(input_data) // batch_size\n",
    "val_steps_per_epoch = len(input_data_val) // batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "MY8CejJgL1CJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1080/1080 [==============================] - 693s 642ms/step - loss: 0.0756 - accuracy: 0.9776 - val_loss: 0.1702 - val_accuracy: 0.9650\n",
      "Epoch 2/10\n",
      "1080/1080 [==============================] - 691s 641ms/step - loss: 0.0678 - accuracy: 0.9801 - val_loss: 0.1664 - val_accuracy: 0.9664\n",
      "Epoch 3/10\n",
      "1080/1080 [==============================] - 689s 639ms/step - loss: 0.0622 - accuracy: 0.9819 - val_loss: 0.1621 - val_accuracy: 0.9672\n",
      "Epoch 4/10\n",
      "1080/1080 [==============================] - 689s 638ms/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.1596 - val_accuracy: 0.9680\n",
      "Epoch 5/10\n",
      "1080/1080 [==============================] - 690s 639ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 0.1579 - val_accuracy: 0.9684\n",
      "Epoch 6/10\n",
      "1080/1080 [==============================] - 687s 637ms/step - loss: 0.0499 - accuracy: 0.9853 - val_loss: 0.1547 - val_accuracy: 0.9699\n",
      "Epoch 7/10\n",
      "1080/1080 [==============================] - 689s 638ms/step - loss: 0.0464 - accuracy: 0.9861 - val_loss: 0.1537 - val_accuracy: 0.9699\n",
      "Epoch 8/10\n",
      "1080/1080 [==============================] - 687s 636ms/step - loss: 0.0441 - accuracy: 0.9866 - val_loss: 0.1527 - val_accuracy: 0.9705\n",
      "Epoch 9/10\n",
      "1080/1080 [==============================] - 690s 640ms/step - loss: 0.0419 - accuracy: 0.9871 - val_loss: 0.1544 - val_accuracy: 0.9705\n",
      "Epoch 10/10\n",
      "1080/1080 [==============================] - 696s 645ms/step - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.1591 - val_accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "# Create a checkpoint object and optimizer\n",
    "checkpoint = tf.train.Checkpoint(my_model=model, optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Restore the checkpoint using the prefix\n",
    "checkpoint_prefix = 'v5_model.ckpt-1'\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# Train the model for another 5 epochs\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_2_layer_call_fn, multi_head_attention_2_layer_call_and_return_conditional_losses, feed_forward_network_2_layer_call_fn, feed_forward_network_2_layer_call_and_return_conditional_losses, layer_normalization_4_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_6/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "tf.saved_model.save(model, 'saved_model_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v4_model.ckpt-1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a checkpoint object and save the model\n",
    "checkpoint = tf.train.Checkpoint(my_model=model)\n",
    "checkpoint.save('v4_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "tu cand te uitai la mine  şchiopete catafalc distind minunato cameii crepusculare pact s'ajungi sunatzi răpit\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature=0.5):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        # Tokenize and pad the input sequence\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "\n",
    "        # Generate predictions\n",
    "        predicted_probs = model.predict(token_list)[0][-1]\n",
    "        #print(predicted_probs.shape)\n",
    "        #print(predicted_probs)\n",
    "\n",
    "\n",
    "        # Apply temperature to the predicted probabilities\n",
    "        exp_preds = np.exp(np.log(predicted_probs) / temperature)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        #print(predicted_probs)\n",
    "\n",
    "        # Normalize the probabilities using softmax\n",
    "        predicted_probs = K.softmax(predicted_probs).numpy()\n",
    "\n",
    "        # Select a word index based on the probability distribution\n",
    "        predicted = np.random.choice(range(total_words), p=predicted_probs)\n",
    "       # print(range(vocab_size))\n",
    "        #print(predicted)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        # Append the generated word to the seed text\n",
    "        seed_text += \" \" + output_word\n",
    "        output_text += \" \" + output_word\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "seed_text = \"tu cand te uitai la mine \"\n",
    "next_words = 10\n",
    "#vocab_size = 19038\n",
    "temperature = 1.2\n",
    "new_lyrics = generate_lyrics(seed_text, next_words, model, max_sequence_len, temperature)\n",
    "print(new_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1080/1080 [==============================] - 496s 459ms/step - loss: 0.0615 - accuracy: 0.9809 - val_loss: 0.1618 - val_accuracy: 0.9677\n",
      "Epoch 2/10\n",
      "1080/1080 [==============================] - 492s 456ms/step - loss: 0.0577 - accuracy: 0.9821 - val_loss: 0.1627 - val_accuracy: 0.9679\n",
      "Epoch 3/10\n",
      "1080/1080 [==============================] - 501s 464ms/step - loss: 0.0543 - accuracy: 0.9831 - val_loss: 0.1612 - val_accuracy: 0.9688\n",
      "Epoch 4/10\n",
      "1080/1080 [==============================] - 498s 462ms/step - loss: 0.0517 - accuracy: 0.9840 - val_loss: 0.1618 - val_accuracy: 0.9686\n",
      "Epoch 5/10\n",
      "1080/1080 [==============================] - 513s 476ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 0.1578 - val_accuracy: 0.9700\n",
      "Epoch 6/10\n",
      "1080/1080 [==============================] - 539s 499ms/step - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.1599 - val_accuracy: 0.9696\n",
      "Epoch 7/10\n",
      "1080/1080 [==============================] - 494s 457ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.1527 - val_accuracy: 0.9705\n",
      "Epoch 8/10\n",
      "1080/1080 [==============================] - 502s 465ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 0.1562 - val_accuracy: 0.9705\n",
      "Epoch 9/10\n",
      "1080/1080 [==============================] - 505s 468ms/step - loss: 0.0421 - accuracy: 0.9867 - val_loss: 0.1542 - val_accuracy: 0.9708\n",
      "Epoch 10/10\n",
      "1080/1080 [==============================] - 499s 463ms/step - loss: 0.0409 - accuracy: 0.9870 - val_loss: 0.1555 - val_accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "# Create a checkpoint object and optimizer\n",
    "checkpoint = tf.train.Checkpoint(my_model=model, optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Restore the checkpoint using the prefix\n",
    "checkpoint_prefix = 'v3_model.ckpt-1'\n",
    "checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "# Train the model for another 5 epochs\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_10_layer_call_fn, multi_head_attention_10_layer_call_and_return_conditional_losses, feed_forward_network_10_layer_call_fn, feed_forward_network_10_layer_call_and_return_conditional_losses, layer_normalization_20_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_4/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "tf.saved_model.save(model, 'saved_model_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
